{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Basics .ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bias Classifier, \n",
        "Bi-grams ans n-grams  \n",
        "NLP (Basics)"
      ],
      "metadata": {
        "id": "GJSXlFGZJrUd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHOvYKRdhDbm"
      },
      "outputs": [],
      "source": [
        "#Importing Librarires  \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk \n",
        "from collections import Counter\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading Packages:  "
      ],
      "metadata": {
        "id": "-DulVtXPe1Q5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xh9JB36sC3cb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " nltk.download('punkt')"
      ],
      "metadata": {
        "id": "6iqGIJU4ncAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "cyRbB_Ur4aim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading Data- Doctor Pateient Conversation 204 sentences "
      ],
      "metadata": {
        "id": "tX-cvro-fImX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_excel(\"Data_doctor.xlsx\")\n",
        "df.head(5)\n",
        "df.columns\n"
      ],
      "metadata": {
        "id": "9CHXaHyLlALx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shape of the data \n",
        "df.shape"
      ],
      "metadata": {
        "id": "1CpQGoQydtvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Description of the data. \n",
        "df.describe()"
      ],
      "metadata": {
        "id": "qhmgHZ4LdxXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PARTS OF SPEECH TAGGING:"
      ],
      "metadata": {
        "id": "Zf2QW0Uoxh5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def tokenzie(Sen):\n",
        "  line = re.sub('[)(?!,.@#$]', '', Sen)\n",
        "  token=nltk.word_tokenize(line)\n",
        "  print(\"POS Tagging:\",nltk.pos_tag(token))\n",
        "\n",
        "for row in df.iterrows():\n",
        "  for i in range(len(df)): \n",
        "    Sen=df[\"Sentences \"].iloc[i]\n",
        "    tokenzie(Sen)"
      ],
      "metadata": {
        "id": "SGKTgyafDuRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most Frequent Words and Non-Frequent Words :"
      ],
      "metadata": {
        "id": "WUFMyGe4h4nZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing CHaracyers from the Sentences \n",
        "for row in df.iterrows():\n",
        "  for i in range(len(df)): \n",
        "    Sen=df[\"Sentences \"].iloc[i]\n",
        "    line = re.sub('[)(?!.@#$]', '', Sen)"
      ],
      "metadata": {
        "id": "kosHjkPe4s-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.savetxt(r'file.txt', df.values, fmt='%s')"
      ],
      "metadata": {
        "id": "TRwopV-lH-1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"file.txt\", \"r\")\n",
        "simple_text=f.read()\n",
        "word_list = simple_text.split()\n",
        "word_count = Counter(word_list)\n",
        "print(word_count.most_common(20))"
      ],
      "metadata": {
        "id": "7WO33ybSIujr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d ={'words':word_count.most_common()}\n",
        "df1=pd.DataFrame(d)\n",
        "df1['words']\n",
        "df2 = pd.DataFrame(df1['words'].tolist(), index=df1.index)\n",
        "df2.rename(columns = {0:'words'}, inplace = True)\n",
        "df2.rename(columns = {1:'counts'}, inplace = True)\n",
        "df3=pd.DataFrame(df2.head(20))\n",
        "df4=pd.DataFrame(df2.tail(10))"
      ],
      "metadata": {
        "id": "7AENr_1bJklf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Most Frequent word \n",
        "\n",
        "  \n",
        "plt.barh(df3['words'], df3['counts'])\n",
        "plt.title('Words vs Frequency')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "03fiWTSsJ_6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#most non frequent word\n",
        "import matplotlib.pyplot as plt\n",
        "  \n",
        "plt.barh(df4['words'], df4['counts'])\n",
        "plt.title('Words vs non-Frequency')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1gO42HgIKJVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "Making a list of Stopwords and removing them:"
      ],
      "metadata": {
        "id": "4Kx5VRw2We56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data={'Stopwords':['the','and','is','are','too','did', 'doing', 'a', 'an', 'the',  'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more']}\n",
        "df_stop=pd.DataFrame(data)\n",
        "df_stop.tail()"
      ],
      "metadata": {
        "id": "NlCigj9mWY9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_stop.shape"
      ],
      "metadata": {
        "id": "0D-Q-BdiiTtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "35YmlLiYDngv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Saving the Stopwords in text file. \n",
        "np.savetxt(r'stopwords.txt', df_stop.values, fmt='%s')\n"
      ],
      "metadata": {
        "id": "3hHzv8RsXkZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"stopwords.txt\", \"r\")\n",
        "text=f.read()"
      ],
      "metadata": {
        "id": "aUxVAzWpXwUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text)"
      ],
      "metadata": {
        "id": "p22HtRitXxNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wISAk_YTHs4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "stpwrd = text\n",
        "text_tokens = word_tokenize(simple_text)"
      ],
      "metadata": {
        "id": "YnsbL_7RYD4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "removing_custom_words = [words for words in text_tokens if not words in stpwrd]\n",
        "print(removing_custom_words)\n"
      ],
      "metadata": {
        "id": "5odPfLHVYxwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total count of words in a sentence is:\",len(text_tokens), \"Total count of custom words:\",\n",
        "len(removing_custom_words))"
      ],
      "metadata": {
        "id": "TjzA_a8UYzHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wordcloud"
      ],
      "metadata": {
        "id": "q1LE26adYk1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"file.txt\", \"r\")\n",
        "text=f.read()\n",
        "data= WordCloud().generate(text)\n",
        "plt.imshow(data, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "5OKi75KUZlt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "UNIGRAM BIGRAM and TRIGRAM \n"
      ],
      "metadata": {
        "id": "1g8BWdV6KgJC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "UNI-GRAM"
      ],
      "metadata": {
        "id": "ktSEcbo5MfCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "\n",
        "n = 1\n",
        "sentence = simple_text\n",
        "unigrams = ngrams(sentence.split(), n)\n",
        "\n",
        "for item in unigrams:\n",
        "    print(item)\n"
      ],
      "metadata": {
        "id": "apMp-oqHJ7oP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BI-GRAM"
      ],
      "metadata": {
        "id": "AmkGkJLIMkrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "\n",
        "n = 2\n",
        "sentence = simple_text\n",
        "unigrams = ngrams(sentence.split(), n)\n",
        "\n",
        "for item in unigrams:\n",
        "    print(item)\n",
        "\n"
      ],
      "metadata": {
        "id": "RYrOL9jbL1B1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRI-GRAM"
      ],
      "metadata": {
        "id": "kfEzuY0XMulN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "\n",
        "n = 3\n",
        "sentence = simple_text\n",
        "unigrams = ngrams(sentence.split(), n)\n",
        "\n",
        "for item in unigrams:\n",
        "    print(item)"
      ],
      "metadata": {
        "id": "U-1FBdXRMuDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bias CLassifer Model Predicition: "
      ],
      "metadata": {
        "id": "zorN_DpQTe5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import mode\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "%matplotlib inline\n"
      ],
      "metadata": {
        "id": "rMxZbnl-OAP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the train.csv by removing the\n",
        "# last column since it's an empty column\n",
        "DATA_PATH = \"Training.csv\"\n",
        "data = pd.read_csv(DATA_PATH).dropna(axis = 1)\n",
        "\n",
        "# Checking whether the dataset is balanced or not\n",
        "disease_counts = data[\"prognosis\"].value_counts()\n",
        "temp_df = pd.DataFrame({\n",
        "\t\"Disease\": disease_counts.index,\n",
        "\t\"Counts\": disease_counts.values\n",
        "})\n",
        "\n",
        "plt.figure(figsize = (18,8))\n",
        "sns.barplot(x = \"Disease\", y = \"Counts\", data = temp_df)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "UdzvPU6cTyuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding the target value into numerical\n",
        "# value using LabelEncoder\n",
        "encoder = LabelEncoder()\n",
        "data[\"prognosis\"] = encoder.fit_transform(data[\"prognosis\"])\n"
      ],
      "metadata": {
        "id": "GgcDS5JIT5Lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.iloc[:,:-1]\n",
        "y = data.iloc[:, -1]\n",
        "X_train, X_test, y_train, y_test =train_test_split(\n",
        "X, y, test_size = 0.2, random_state = 24)\n",
        "\n",
        "print(f\"Train: {X_train.shape}, {y_train.shape}\")\n",
        "print(f\"Test: {X_test.shape}, {y_test.shape}\")\n"
      ],
      "metadata": {
        "id": "jdxy5hn-UCi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining scoring metric for k-fold cross validation\n",
        "def cv_scoring(estimator, X, y):\n",
        "\treturn accuracy_score(y, estimator.predict(X))\n",
        "\n",
        "# Initializing Models\n",
        "models = {\n",
        "\t\"SVC\":SVC(),\n",
        "}\n",
        "\n",
        "# Producing cross validation score for the models\n",
        "for model_name in models:\n",
        "\tmodel = models[model_name]\n",
        "\tscores = cross_val_score(model, X, y, cv = 10,\n",
        "\t\t\t\t\t\t\tn_jobs = -1,\n",
        "\t\t\t\t\t\t\tscoring = cv_scoring)\n",
        "\tprint(\"==\"*30)\n",
        "\tprint(model_name)\n",
        "\tprint(f\"Scores: {scores}\")\n",
        "\tprint(f\"Mean Score: {np.mean(scores)}\")\n"
      ],
      "metadata": {
        "id": "LuNBVcLGUG8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and testing SVM Classifier\n",
        "svm_model = SVC()\n",
        "svm_model.fit(X_train, y_train)\n",
        "preds = svm_model.predict(X_test)\n",
        "\n",
        "print(f\"Accuracy on train data by SVM Classifier\\\n",
        ": {accuracy_score(y_train, svm_model.predict(X_train))*100}\")\n",
        "\n",
        "print(f\"Accuracy on test data by SVM Classifier\\\n",
        ": {accuracy_score(y_test, preds)*100}\")\n",
        "cf_matrix = confusion_matrix(y_test, preds)\n",
        "plt.figure(figsize=(12,8))\n",
        "sns.heatmap(cf_matrix, annot=True)\n",
        "plt.title(\"Confusion Matrix for SVM Classifier on Test Data\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "WipWSaXoMypg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the models on whole data\n",
        "final_svm_model = SVC()\n",
        "final_svm_model.fit(X, y)\n",
        "\n",
        "# Reading the test data\n",
        "test_data = pd.read_csv(\"Testing.csv\").dropna(axis=1)\n",
        "\n",
        "test_X = test_data.iloc[:, :-1]\n",
        "test_Y = encoder.transform(test_data.iloc[:, -1])\n",
        "\n",
        "# Making prediction by take mode of predictions\n",
        "# made by all the classifiers\n",
        "svm_preds = final_svm_model.predict(test_X)\n",
        "\n",
        "\n",
        "final_preds = [mode([i])[0][0] for i\n",
        "\t\t\t in zip(svm_preds)]\n",
        "\n",
        "print(f\"Accuracy on Test dataset by the combined model\\\n",
        ": {accuracy_score(test_Y, final_preds)*100}\")\n",
        "\n",
        "cf_matrix = confusion_matrix(test_Y, final_preds)\n",
        "plt.figure(figsize=(12,8))\n",
        "\n",
        "sns.heatmap(cf_matrix, annot = True)\n",
        "plt.title(\"Confusion Matrix for Combined Model on Test Dataset\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HWethtT1WkfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "symptoms = X.columns.values\n",
        "\n",
        "# Creating a symptom index dictionary to encode the\n",
        "# input symptoms into numerical form\n",
        "symptom_index = {}\n",
        "for index, value in enumerate(symptoms):\n",
        "\tsymptom = \" \".join([i.capitalize() for i in value.split(\"_\")])\n",
        "\tsymptom_index[symptom] = index\n",
        "\n",
        "data_dict = {\n",
        "\t\"symptom_index\":symptom_index,\n",
        "\t\"predictions_classes\":encoder.classes_\n",
        "}\n",
        "\n",
        "# Defining the Function\n",
        "# Input: string containing symptoms separated by commmas\n",
        "# Output: Generated predictions by models\n",
        "def predictDisease(symptoms):\n",
        "\tsymptoms = symptoms.split(\",\")\n",
        "\t\n",
        "\t# creating input data for the models\n",
        "\tinput_data = [0] * len(data_dict[\"symptom_index\"])\n",
        "\tfor symptom in symptoms:\n",
        "\t\tindex = data_dict[\"symptom_index\"][symptom]\n",
        "\t\tinput_data[index] = 1\n",
        "\t\t\n",
        "\t# reshaping the input data and converting it\n",
        "\t# into suitable format for model predictions\n",
        "\tinput_data = np.array(input_data).reshape(1,-1)\n",
        "\t\n",
        "\t# generating individual outputs\n",
        "\n",
        "\tsvm_prediction = data_dict[\"predictions_classes\"][final_svm_model.predict(input_data)[0]]\n",
        "\t\n",
        "\t# making final prediction by taking mode of all predictions\n",
        "\tfinal_prediction = mode([svm_prediction])[0][0]\n",
        "\tpredictions = {\n",
        "\t\t\n",
        "\t\t\"final_prediction\":final_prediction\n",
        "\t}\n",
        "\treturn predictions\n",
        "\n",
        "# Testing the function\n",
        "print(predictDisease(\"Itching,Skin Rash,Nodal Skin Eruptions\"))\n"
      ],
      "metadata": {
        "id": "FBVvVZVAWVQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install Wordsense"
      ],
      "metadata": {
        "id": "H47X-SehD0BQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.wsd import lesk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "-DvwL-PLFHro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = {'Word': [], 'Sysnet': [], 'Definition':[]}\n",
        "df_new = pd.DataFrame(data=d)"
      ],
      "metadata": {
        "id": "AiSOfQLgJDsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"file.txt\", \"r\")\n",
        "simple_text=f.read()"
      ],
      "metadata": {
        "id": "iWXYLj0ZGL-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = ''"
      ],
      "metadata": {
        "id": "QOlnqgWXJRl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t in simple_text:\n",
        "    output += t"
      ],
      "metadata": {
        "id": "1Ml3UCr9JRy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gram1 = nltk.word_tokenize(output)\n",
        "gram1= [word.lower() for word in gram1 if word.isalnum()]\n",
        "\n",
        "res = []\n",
        "[res.append(x) for x in gram1 if x not in res]\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "res2 = []\n",
        "[res2.append(x) for x in res if x not in stop_words]\n",
        "\n",
        "res2 = nltk.pos_tag(res2)"
      ],
      "metadata": {
        "id": "F4CHErfEJZFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x in res2:\n",
        "  try:\n",
        "    if 'N' in x[1]:\n",
        "      pos = 'n'\n",
        "    if 'V' in x[1]:\n",
        "      pos = 'v'\n",
        "    if 'J' in x[1]:\n",
        "      pos = 'a'\n",
        "    if 'R' in x[1]:\n",
        "      pos = 'r'\n",
        "    if 'I' in x[1]:\n",
        "      pos = 'r'\n",
        "    new_row = {\"Word\":x[0],\"Sysnet\":str(lesk(gram1,x[0],pos)),\"Definition\":str(lesk(gram1,x[0],pos).definition())}\n",
        "    df = df.append(new_row, ignore_index=True)\n",
        "  except:\n",
        "    pass\n",
        "df2=df[['Word','Sysnet','Definition']]\n",
        "df2.tail(50)"
      ],
      "metadata": {
        "id": "7HrzCRsGJd1w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}